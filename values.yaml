alertmanager:
  name: alertmanager
  serviceType: ClusterIP
  persistentVolume:
    ## If true, alertmanager will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## alertmanager data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce
  

    ## alertmanager data Persistent Volume Claim annotations
    ##
    annotations: {}

    ## alertmanager data Persistent Volume existing claim name
    ## Requires alertmanager.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    ## alertmanager data Persistent Volume mount root path
    ##
    mountPath: /data

    ## alertmanager data Persistent Volume size
    ##
    size: 2Gi

    ## alertmanager data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"
  ## Alertmanager service port
  
  httpPort: 80

  ## Alertmanager service port name
  ## Default: 'http'
  ##
  # httpPortName: http

  ## Alertmanager Docker image
  ##
  image: prom/alertmanager:v0.5.1
  resources:
    limits:
      cpu: 10m
      memory: 32Mi
    requests:
      cpu: 10m
      memory: 32Mi
namespace: monitoring 
  
## Alertmanager data storage path
## Default: '/data'
# storagePath: /data

## Monitors ConfigMap changes and POSTs to a URL
## Ref: https://github.com/jimmidyson/configmap-reload
##
configmapReload:
  ## Configmap-reload Docker image
  ##
  image: jimmidyson/configmap-reload:v0.1

  ## Configmap-reload container name
  ##
  name: configmap-reload

## Global imagePullPolicy
## Default: 'Always' if image tag is 'latest', else 'IfNotPresent'
## Ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
##
# imagePullPolicy:

kubeStateMetrics:
  ## Kube-state-metrics service port
  httpPort: 80

  ## Kube-state-metrics service port name
  ## Default: 'http'
  ##
  # httpPortName: http

  ## Kube-state-metrics Docker image
  ##
  image: gcr.io/google_containers/kube-state-metrics:v0.3.0

  ## Kube-state-metrics container name
  ##
  name: kube-state-metrics

  ## Kube-state-metrics resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    # limits:
    #   cpu: 10m
    #   memory: 16Mi
    requests:
      cpu: 10m
      memory: 16Mi

  ## Kube-state-metrics service type
  ##
  serviceType: ClusterIP

server:
  ## Server Pod annotations:
  ##
  # annotations:
  #   iam.amazonaws.com/role: prometheus

  ## Additional Server container arguments
  ##
  # extraArgs:

  ## Server service port
  ##
  httpPort: 80

  ## Server service port name
  ## Default: 'http'
  ##
  # httpPortName: http

  ## Server Docker image
  ##
  image: prom/prometheus:v1.4.1

  ingress:
    ## If true, Server Ingress will be created
    ##
    enabled: false

    ## Server Ingress annotations
    ##
    # annotations:
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    ## Server Ingress hostnames
    ## Must be provided if Ingress is enabled
    ##
    # hosts:
    #   - prometheus.domain.com

    ## Server Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    # tls:
    #   - secretName: prometheus-server-tls
    #     hosts:
    #       - prometheus.domain.com

  ## Server container name
  ##
  name: server

  persistentVolume:
    ## If true, Server will create a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## Server data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## Server data Persistent Volume annotations
    ##
    # annotations:

    ## Server data Persistent Volume size
    ##
    size: 8Gi

    ## AlertManager data Persistent Volume Storage Class
    ## If defined, volume.beta.kubernetes.io/storage-class: <storageClass>
    ## Default: volume.alpha.kubernetes.io/storage-class: default
    ##
    # storageClass:

  ## Server resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    # limits:
    #   cpu: 500m
    #   memory: 512Mi
    requests:
      cpu: 500m
      memory: 512Mi

  ## Server service type
  ##
  serviceType: ClusterIP

  ## Server local data storage path
  ## Default: '/data'
  ##
  # storageLocalPath: /data

  ## Server Pod termination grace period
  ## Default: 300s (5m)
  ##
  # terminationGracePeriodSeconds: 300

## Alertmanager ConfigMap entries
##
alertmanagerFiles:
  alertmanager.yml: |
    global:
      # slack_api_url: ''

    receivers:
      - name: default-receiver
        # slack_configs:
        #  - channel: '@you'
        #    send_resolved: true

    route:
      group_wait: 10s
      group_interval: 5m
      receiver: default-receiver
      repeat_interval: 3h

## Server ConfigMap entries
##
serverFiles:
  alerts: ""
  rules: ""

  prometheus.yml: |
    global:
      scrape_interval:     15s
      evaluation_interval: 15s

    rule_files:
      - /etc/config/rules
      - /etc/config/alerts
    alerting:
      alertmanagers:
      - scheme: http
        static_configs:
        - targets:
          - "alertmanager.monitoring.svc:9093"

    scrape_configs:
      - job_name: prometheus
        static_configs:
          - targets:
            - http://prometheus-mon.monitoring.svc:9090

      # Scrape configurations for running Prometheus on a Kubernetes cluster.
      # This uses separate scrape configs for cluster components (i.e. API server, node)
      # and services to allow each to use different authentication configs.
      #
      # Kubernetes labels will be added as Prometheus labels on metrics via the
      # `labelmap` relabeling action.

      # Scrape config for API servers.
      - job_name: kubernetes-apiservers

        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https

        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration (`in_cluster` below) because discovery & scraping are two
        # separate concerns in Prometheus.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          # insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        # Keep only the default/kubernetes service endpoints for the https port. This
        # will add targets for each API server which Kubernetes adds an endpoint to
        # the default/kubernetes service.
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: default;kubernetes;https

      - job_name: kubernetes-nodes

        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https

        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration (`in_cluster` below) because discovery & scraping are two
        # separate concerns in Prometheus.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          # insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
          - role: node

        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)

      # Scrape config for service endpoints.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: If the metrics are exposed on a different port to the
      # service then set this appropriately.
      - job_name: 'kubernetes-service-endpoints'

        kubernetes_sd_configs:
          - role: endpoints

        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: (.+)(?::\d+);(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_service_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: kubernetes_name

      # Example scrape config for probing services via the Blackbox Exporter.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/probe`: Only probe services that have a value of `true`
      # - job_name: 'kubernetes-services'
      #
      #   metrics_path: /probe
      #   params:
      #     module: [http_2xx]
      #
      #   kubernetes_sd_configs:
      #     - role: service
      #
      #   relabel_configs:
      #     - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
      #       action: keep
      #       regex: true
      #     - source_labels: [__address__]
      #       target_label: __param_target
      #     - target_label: __address__
      #       replacement: blackbox
      #     - source_labels: [__param_target]
      #       target_label: instance
      #     - action: labelmap
      #       regex: __meta_kubernetes_service_label_(.+)
      #     - source_labels: [__meta_kubernetes_service_namespace]
      #       target_label: kubernetes_namespace
      #     - source_labels: [__meta_kubernetes_service_name]
      #       target_label: kubernetes_name

      # Example scrape config for pods
      #
      # The relabeling allows the actual pod scrape endpoint to be configured via the
      # following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
      - job_name: 'kubernetes-pods'

        kubernetes_sd_configs:
          - role: pod

        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: (.+):(?:\d+);(\d+)
            replacement: ${1}:${2}
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_pod_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name
server:
  ## Grafana Pod annotations:
  ##
  # annotations:
  #   iam.amazonaws.com/role: grafana

  ## Grafana service port
  ##
  httpPort: 80

  ## Grafana Docker image
  ##
  image: "grafana/grafana:latest"

  ingress:
    ## If true, Grafana Ingress will be created
    ##
    enabled: false

    ## Grafana Ingress annotations
    ##
    # annotations:
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    ## Grafana Ingress hostnames
    ## Must be provided if Ingress is enabled
    ##
    # hosts:
    #   - grafana.domain.com

    ## Grafana Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    # tls:
    #   - secretName: grafana-server-tls
    #     hosts:
    #       - grafana.domain.com

  ## Grafana container name
  ##
  name: grafana

  adminUser: YWRtaW4=
  adminPassword: YWRtaW4=

  ## Global imagePullPolicy
  ## Default: 'Always' if image tag is 'latest', else 'IfNotPresent'
  ## Ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  ##
  # imagePullPolicy:

  # Persist data to a persitent volume
  persistentVolume:
    ## If true, Grafana will create a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## Grafana data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## Server data Persistent Volume annotations
    ##
    # annotations:

    ## Grafana data Persistent Volume size
    ## Default: 1Gi
    ##
    size: 1Gi

    ## Data Persistent Volume Storage Class
    ## If defined, volume.beta.kubernetes.io/storage-class: <storageClass>
    ## Default: volume.alpha.kubernetes.io/storage-class: default
    ##
    # storageClass:

  ## Grafana resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    # limits:
    #   cpu: 500m
    #   memory: 512Mi
    requests:
      cpu: 100m
      memory: 100Mi

  ## Grafana service type
  ##
  serviceType: LoadBalancer

  ## Grafana local config path
  ## Default '/etc/grafana'
  ##
  # configLocalPath: /etc/grafana

  ## Grafana local dashboards path
  ## Default: '/var/lib/grafana/dashboards'
  ##
  # dashboardLocalPath: /var/lib/grafana/dashboards

  ## Grafana local data storage path
  ## Default: '/var/lib/grafana/data'
  ##
  # storageLocalPath: /var/lib/grafana/data

  ## Grafana Pod termination grace period
  ## Default: 300s (5m)
  ##
  # terminationGracePeriodSeconds: 300

  # Set datasource in beginning
  setDatasource:
    ## If true, an initial Grafana Datasource will be set
    ## Default: false
    ##
    enabled: false

    ## How long should it take to commit failure
    ## Default: 300
    ##
    # activeDeadlineSeconds: 300

    ## Curl Docker image
    ## Default: appropriate/curl:latest
    ##
    # image: appropriate/curl:latest

    ## This assembles how curl post into Grafana
    ## Ref1: http://docs.grafana.org/reference/http_api/#create-data-source
    ## Ref2: https://github.com/grafana/grafana/issues/1789
    ##
    # datasource:
      ## The datasource name.
      ## Default: default
      # name: default

      ## Type of datasource
      ## Default: prometheus
      ##
      # type: prometheus

      ## The url of the datasource. To set correctly you need to know
      ## the right datasource name and its port ahead. Check kubernetes
      ## dashboard or describe the service should fulfill the requirements.
      ## Synatx like `http://<release name>-<server name>:<port number>
      ## Default: "http://limping-tiger-server"
      ##
      # url: "http://limping-tiger-server"

      ## Specify if Grafana has to go thru proxy to reach datasource
      ## Default: proxy
      ##
      # access: proxy

      ## Specify should Grafana use this datasource as default
      ## Default: true
      ##
      # isDefault: true

    ## Specify the job policy
    ## Default: OnFailure
    ##
    # restartPolicy: OnFailure

## Grafana config file ConfigMap entry
##
serverConfigFile:
  grafana.ini: |
    ; instance_name = ${HOSTNAME}
    [paths]
    data = /var/lib/grafana/data
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins

    [server]
    ;protocol = http
    ;http_addr =
    ;http_port = 3000
    ;domain = localhost
    ;enforce_domain = false
    ;root_url = %(protocol)s://%(domain)s:%(http_port)s/
    ;router_logging = false
    ;static_root_path = public
    ;enable_gzip = false
    ;cert_file =
    ;cert_key =

    [database]
    ;type = sqlite3
    ;host = 127.0.0.1:3306
    ;name = grafana
    ;user = root
    ;password =
    ;ssl_mode = disable
    ;path = grafana.db

    [session]
    ;provider = file
    ;provider_config = sessions
    ;cookie_name = grafana_sess
    ;cookie_secure = false
    ;session_life_time = 86400

    [analytics]
    ;reporting_enabled = true
    check_for_updates = true
    ;google_analytics_ua_id =

    [security]
    ;admin_user = admin
    ;admin_password = admin
    ;secret_key = SW2YcwTIb9zpOOhoPsMm
    ;login_remember_days = 7
    ;cookie_username = grafana_user
    ;cookie_remember_name = grafana_remember
    ;disable_gravatar = false
    ;data_source_proxy_whitelist =

    [snapshots]
    ;external_enabled = true
    ;external_snapshot_url = https://snapshots-origin.raintank.io
    ;external_snapshot_name = Publish to snapshot.raintank.io

    [users]
    ;allow_sign_up = true
    ;allow_org_create = true
    ;auto_assign_org = true
    ;auto_assign_org_role = Viewer
    ;login_hint = email or username
    ;default_theme = dark

    [auth.anonymous]
    ;enabled = false
    ;org_name = Main Org.
    ;org_role = Viewer

    [auth.github]
    ;enabled = false
    ;allow_sign_up = false
    ;client_id = some_id
    ;client_secret = some_secret
    ;scopes = user:email,read:org
    ;auth_url = https://github.com/login/oauth/authorize
    ;token_url = https://github.com/login/oauth/access_token
    ;api_url = https://api.github.com/user
    ;team_ids =
    ;allowed_organizations =

    [auth.google]
    ;enabled = false
    ;allow_sign_up = false
    ;client_id = some_client_id
    ;client_secret = some_client_secret
    ;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email
    ;auth_url = https://accounts.google.com/o/oauth2/auth
    ;token_url = https://accounts.google.com/o/oauth2/token
    ;api_url = https://www.googleapis.com/oauth2/v1/userinfo
    ;allowed_domains =

    [auth.proxy]
    ;enabled = false
    ;header_name = X-WEBAUTH-USER
    ;header_property = username
    ;auto_sign_up = true

    [auth.basic]
    ;enabled = true

    [auth.ldap]
    ;enabled = false
    ;config_file = /etc/grafana/ldap.toml

    [smtp]
    ;enabled = false
    ;host = localhost:25
    ;user =
    ;password =
    ;cert_file =
    ;key_file =
    ;skip_verify = false
    ;from_address = admin@grafana.localhost

    [emails]
    ;welcome_email_on_sign_up = false

    [log]
    mode = console
    level = info

    [log.console]
    ;level =
    ;format = console

    [event_publisher]
    ;enabled = false
    ;rabbitmq_url = amqp://localhost/
    ;exchange = grafana_events

    [dashboards.json]
    enabled = true
    path = /var/lib/grafana/dashboards

    [metrics]
    ;enabled           = true
    ;interval_seconds  = 10

    ; [metrics.graphite]
    ; address = localhost:2003
    ; prefix = prod.grafana.%(instance_name)s.

    [grafana_net]
    url = https://grafana.net


## Grafana dashboard files ConfigMap entries
## If you'd like to preinstall prometheus dashboard on the same namespace as example, get it from:
##
## https://grafana.net/dashboards/2
##
## and add it below.
##
serverDashboardFiles: